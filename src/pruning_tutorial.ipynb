{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning Tutorial\n",
    "================\n",
    "\n",
    "**Author**: [Michela Paganini](https://github.com/mickypaganini)\n",
    "\n",
    "State-of-the-art deep learning techniques rely on over-parametrized\n",
    "models that are hard to deploy. On the contrary, biological neural\n",
    "networks are known to use efficient sparse connectivity. Identifying\n",
    "optimal techniques to compress models by reducing the number of\n",
    "parameters in them is important in order to reduce memory, battery, and\n",
    "hardware consumption without sacrificing accuracy. This in turn allows\n",
    "you to deploy lightweight models on device, and guarantee privacy with\n",
    "private on-device computation. On the research front, pruning is used to\n",
    "investigate the differences in learning dynamics between\n",
    "over-parametrized and under-parametrized networks, to study the role of\n",
    "lucky sparse subnetworks and initializations (\\\"[lottery\n",
    "tickets](https://arxiv.org/abs/1803.03635)\\\") as a destructive neural\n",
    "architecture search technique, and more.\n",
    "\n",
    "In this tutorial, you will learn how to use `torch.nn.utils.prune` to\n",
    "sparsify your neural networks, and how to extend it to implement your\n",
    "own custom pruning technique.\n",
    "\n",
    "Requirements\n",
    "------------\n",
    "\n",
    "`\"torch>=1.4.0a0+8e8a5e0\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                            train = True,\n",
    "                                            transform = transforms.Compose([\n",
    "                                                    transforms.Resize((32,32)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                            download = False)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                            train = False,\n",
    "                                            transform = transforms.Compose([\n",
    "                                                    transforms.Resize((32,32)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                            download=False)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model\n",
    "==============\n",
    "\n",
    "In this tutorial, we use the\n",
    "[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture\n",
    "from LeCun et al., 1998.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    ".format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a Module\n",
    "================\n",
    "\n",
    "Let\\'s inspect the (unpruned) `conv1` layer in our LeNet model. It will\n",
    "contain two parameters `weight` and `bias`, and no buffers, for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning a Module\n",
    "================\n",
    "\n",
    "To prune a module (in this example, the `conv1` layer of our LeNet\n",
    "architecture), first select a pruning technique among those available in\n",
    "`torch.nn.utils.prune` (or\n",
    "[implement](#extending-torch-nn-utils-pruning-with-custom-pruning-functions)\n",
    "your own by subclassing `BasePruningMethod`). Then, specify the module\n",
    "and the name of the parameter to prune within that module. Finally,\n",
    "using the adequate keyword arguments required by the selected pruning\n",
    "technique, specify the pruning parameters.\n",
    "\n",
    "In this example, we will prune at random 30% of the connections in the\n",
    "parameter named `weight` in the `conv1` layer. The module is passed as\n",
    "the first argument to the function; `name` identifies the parameter\n",
    "within that module using its string identifier; and `amount` indicates\n",
    "either the percentage of connections to prune (if it is a float between\n",
    "0. and 1.), or the absolute number of connections to prune (if it is a\n",
    "non-negative integer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning acts by removing `weight` from the parameters and replacing it\n",
    "with a new parameter called `weight_orig` (i.e. appending `\"_orig\"` to\n",
    "the initial parameter `name`). `weight_orig` stores the unpruned version\n",
    "of the tensor. The `bias` was not pruned, so it will remain intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pruning mask generated by the pruning technique selected above is\n",
    "saved as a module buffer named `weight_mask` (i.e. appending `\"_mask\"`\n",
    "to the initial parameter `name`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the forward pass to work without modification, the `weight`\n",
    "attribute needs to exist. The pruning techniques implemented in\n",
    "`torch.nn.utils.prune` compute the pruned version of the weight (by\n",
    "combining the mask with the original parameter) and store them in the\n",
    "attribute `weight`. Note, this is no longer a parameter of the `module`,\n",
    "it is now simply an attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pruning is applied prior to each forward pass using PyTorch\\'s\n",
    "`forward_pre_hooks`. Specifically, when the `module` is pruned, as we\n",
    "have done here, it will acquire a `forward_pre_hook` for each parameter\n",
    "associated with it that gets pruned. In this case, since we have so far\n",
    "only pruned the original parameter named `weight`, only one hook will be\n",
    "present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we can now prune the `bias` too, to see how the\n",
    "parameters, buffers, hooks, and attributes of the `module` change. Just\n",
    "for the sake of trying out another pruning technique, here we prune the\n",
    "3 smallest entries in the bias by L1 norm, as implemented in the\n",
    "`l1_unstructured` pruning function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prune.l1_unstructured(module, name=\"bias\", amount=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now expect the named parameters to include both `weight_orig` (from\n",
    "before) and `bias_orig`. The buffers will include `weight_mask` and\n",
    "`bias_mask`. The pruned versions of the two tensors will exist as module\n",
    "attributes, and the module will now have two `forward_pre_hooks`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative Pruning\n",
    "=================\n",
    "\n",
    "The same parameter in a module can be pruned multiple times, with the\n",
    "effect of the various pruning calls being equal to the combination of\n",
    "the various masks applied in series. The combination of a new mask with\n",
    "the old mask is handled by the `PruningContainer`\\'s `compute_mask`\n",
    "method.\n",
    "\n",
    "Say, for example, that we now want to further prune `module.weight`,\n",
    "this time using structured pruning along the 0th axis of the tensor (the\n",
    "0th axis corresponds to the output channels of the convolutional layer\n",
    "and has dimensionality 6 for `conv1`), based on the channels\\' L2 norm.\n",
    "This can be achieved using the `ln_structured` function, with `n=2` and\n",
    "`dim=0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "# As we can verify, this will zero out all the connections corresponding to \n",
    "# 50% (3 out of 6) of the channels, while preserving the action of the \n",
    "# previous mask.\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding hook will now be of type\n",
    "`torch.nn.utils.prune.PruningContainer`, and will store the history of\n",
    "pruning applied to the `weight` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
    "        break\n",
    "\n",
    "print(list(hook))  # pruning history in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing a pruned model\n",
    "==========================\n",
    "\n",
    "All relevant tensors, including the mask buffers and the original\n",
    "parameters used to compute the pruned tensors are stored in the model\\'s\n",
    "`state_dict` and can therefore be easily serialized and saved, if\n",
    "needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove pruning re-parametrization\n",
    "=================================\n",
    "\n",
    "To make the pruning permanent, remove the re-parametrization in terms of\n",
    "`weight_orig` and `weight_mask`, and remove the `forward_pre_hook`, we\n",
    "can use the `remove` functionality from `torch.nn.utils.prune`. Note\n",
    "that this doesn\\'t undo the pruning, as if it never happened. It simply\n",
    "makes it permanent, instead, by reassigning the parameter `weight` to\n",
    "the model parameters, in its pruned version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to removing the re-parametrization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the re-parametrization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prune.remove(module, 'weight')\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning multiple parameters in a model\n",
    "======================================\n",
    "\n",
    "By specifying the desired pruning technique and parameters, we can\n",
    "easily prune multiple tensors in a network, perhaps according to their\n",
    "type, as we will see in this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_model = LeNet()\n",
    "for name, module in new_model.named_modules():\n",
    "    # prune 20% of connections in all 2D-conv layers \n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "    # prune 40% of connections in all linear layers \n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "\n",
    "print(dict(new_model.named_buffers()).keys())  # to verify that all masks exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global pruning\n",
    "==============\n",
    "\n",
    "So far, we only looked at what is usually referred to as \\\"local\\\"\n",
    "pruning, i.e. the practice of pruning tensors in a model one by one, by\n",
    "comparing the statistics (weight magnitude, activation, gradient, etc.)\n",
    "of each entry exclusively to the other entries in that tensor. However,\n",
    "a common and perhaps more powerful technique is to prune the model all\n",
    "at once, by removing (for example) the lowest 20% of connections across\n",
    "the whole model, instead of removing the lowest 20% of connections in\n",
    "each layer. This is likely to result in different pruning percentages\n",
    "per layer. Let\\'s see how to do that using `global_unstructured` from\n",
    "`torch.nn.utils.prune`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = LeNet()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    "    (model.fc3, 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the sparsity induced in every pruned parameter, which\n",
    "will not be equal to 20% in each layer. However, the global sparsity\n",
    "will be (approximately) 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.conv1.weight == 0))\n",
    "        / float(model.conv1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.conv2.weight == 0))\n",
    "        / float(model.conv2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc1.weight == 0))\n",
    "        / float(model.fc1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc2.weight == 0))\n",
    "        / float(model.fc2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc3.weight == 0))\n",
    "        / float(model.fc3.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.weight == 0)\n",
    "            + torch.sum(model.conv2.weight == 0)\n",
    "            + torch.sum(model.fc1.weight == 0)\n",
    "            + torch.sum(model.fc2.weight == 0)\n",
    "            + torch.sum(model.fc3.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.weight.nelement()\n",
    "            + model.conv2.weight.nelement()\n",
    "            + model.fc1.weight.nelement()\n",
    "            + model.fc2.weight.nelement()\n",
    "            + model.fc3.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending `torch.nn.utils.prune` with custom pruning functions\n",
    "==============================================================\n",
    "\n",
    "To implement your own pruning function, you can extend the\n",
    "`nn.utils.prune` module by subclassing the `BasePruningMethod` base\n",
    "class, the same way all other pruning methods do. The base class\n",
    "implements the following methods for you: `__call__`, `apply_mask`,\n",
    "`apply`, `prune`, and `remove`. Beyond some special cases, you\n",
    "shouldn\\'t have to reimplement these methods for your new pruning\n",
    "technique. You will, however, have to implement `__init__` (the\n",
    "constructor), and `compute_mask` (the instructions on how to compute the\n",
    "mask for the given tensor according to the logic of your pruning\n",
    "technique). In addition, you will have to specify which type of pruning\n",
    "this technique implements (supported options are `global`, `structured`,\n",
    "and `unstructured`). This is needed to determine how to combine masks in\n",
    "the case in which pruning is applied iteratively. In other words, when\n",
    "pruning a prepruned parameter, the current pruning technique is expected\n",
    "to act on the unpruned portion of the parameter. Specifying the\n",
    "`PRUNING_TYPE` will enable the `PruningContainer` (which handles the\n",
    "iterative application of pruning masks) to correctly identify the slice\n",
    "of the parameter to prune.\n",
    "\n",
    "Let\\'s assume, for example, that you want to implement a pruning\n",
    "technique that prunes every other entry in a tensor (or \\-- if the\n",
    "tensor has previously been pruned \\-- in the remaining unpruned portion\n",
    "of the tensor). This will be of `PRUNING_TYPE='unstructured'` because it\n",
    "acts on individual connections in a layer and not on entire\n",
    "units/channels (`'structured'`), or across different parameters\n",
    "(`'global'`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FooBarPruningMethod(prune.BasePruningMethod):\n",
    "    \"\"\"Prune every other entry in a tensor\n",
    "    \"\"\"\n",
    "    PRUNING_TYPE = 'unstructured'\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0 \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to apply this to a parameter in an `nn.Module`, you should also\n",
    "provide a simple function that instantiates the method and applies it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def foobar_unstructured(module, name):\n",
    "    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n",
    "    by removing every other entry in the tensors.\n",
    "    Modifies module in place (and also return the modified module) \n",
    "    by:\n",
    "    1) adding a named buffer called `name+'_mask'` corresponding to the \n",
    "    binary mask applied to the parameter `name` by the pruning method.\n",
    "    The parameter `name` is replaced by its pruned version, while the \n",
    "    original (unpruned) parameter is stored in a new parameter named \n",
    "    `name+'_orig'`.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module containing the tensor to prune\n",
    "        name (string): parameter name within `module` on which pruning\n",
    "                will act.\n",
    "\n",
    "    Returns:\n",
    "        module (nn.Module): modified (i.e. pruned) version of the input\n",
    "            module\n",
    "    \n",
    "    Examples:\n",
    "        >>> m = nn.Linear(3, 4)\n",
    "        >>> foobar_unstructured(m, name='bias')\n",
    "    \"\"\"\n",
    "    FooBarPruningMethod.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s try it out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "foobar_unstructured(model.fc3, name='bias')\n",
    "\n",
    "print(model.fc3.bias_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'lenet_model_pruned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('lenet_model_pruned.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias_orig Parameter containing:\n",
      "tensor([ 0.0273, -0.0799, -0.2132, -0.0352, -0.0541,  0.0110],\n",
      "       requires_grad=True)\n",
      "conv1.weight_orig Parameter containing:\n",
      "tensor([[[[ 0.2347, -0.2508, -0.2891, -0.0000, -0.4715],\n",
      "          [-0.1227, -0.0000, -0.0000, -0.1398, -0.0372],\n",
      "          [-0.0000,  0.1665,  0.0000,  0.2497,  0.3113],\n",
      "          [ 0.1056,  0.1447,  0.1709,  0.2304,  0.1947],\n",
      "          [ 0.2565,  0.0183,  0.0000, -0.1094,  0.0781]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2697,  0.0000,  0.0531, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.2975,  0.3318,  0.3166],\n",
      "          [-0.2586,  0.0000,  0.0909,  0.0227,  0.0000],\n",
      "          [-0.4317, -0.3856, -0.0381, -0.2053, -0.1951],\n",
      "          [-0.1484, -0.2378, -0.4146, -0.1858,  0.0534]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1828,  0.3046,  0.2016, -0.0634, -0.2272],\n",
      "          [ 0.0225,  0.2147, -0.0000, -0.0000, -0.2406],\n",
      "          [ 0.3349,  0.0015, -0.1461, -0.0648, -0.3421],\n",
      "          [ 0.1738,  0.0651, -0.0243,  0.0134, -0.1521],\n",
      "          [ 0.0332,  0.0566, -0.0000, -0.1844, -0.0324]]]], requires_grad=True)\n",
      "conv2.bias Parameter containing:\n",
      "tensor([-0.0130, -0.1662, -0.1204, -0.0718, -0.0661,  0.0741, -0.0350, -0.0936,\n",
      "        -0.0059, -0.1302, -0.0459, -0.0012, -0.0563,  0.0571,  0.0011, -0.1884],\n",
      "       requires_grad=True)\n",
      "conv2.weight_orig Parameter containing:\n",
      "tensor([[[[-2.0301e-01, -7.6507e-02, -1.6983e-01, -1.1914e-01, -5.8392e-02],\n",
      "          [-2.9975e-01, -7.2322e-02, -2.5469e-02,  1.1925e-01,  5.4454e-02],\n",
      "          [ 8.9047e-03,  1.1624e-01,  3.2691e-01,  1.8768e-01, -1.1240e-01],\n",
      "          [ 1.1791e-01,  1.6221e-01,  1.2990e-01, -1.0638e-01, -1.9863e-01],\n",
      "          [ 7.7790e-02, -1.0749e-01, -1.4236e-01,  1.6908e-03,  1.4643e-02]],\n",
      "\n",
      "         [[-5.4062e-02, -4.1418e-03, -8.6665e-02, -6.9553e-02,  1.1236e-01],\n",
      "          [-2.3721e-01, -1.4152e-01, -8.6380e-02,  5.2948e-02,  1.6557e-01],\n",
      "          [-2.5821e-01, -1.1333e-01,  3.8915e-02,  1.1059e-01,  8.3324e-02],\n",
      "          [-7.0682e-02,  7.8025e-02,  6.3106e-02, -2.2819e-02, -1.9597e-01],\n",
      "          [ 9.6535e-02,  1.9822e-01, -3.8993e-02, -3.4484e-02, -1.6820e-01]],\n",
      "\n",
      "         [[ 1.2982e-01, -4.1756e-02, -9.5099e-02, -1.2108e-02,  4.1332e-02],\n",
      "          [-2.4211e-01, -2.7593e-01, -1.9812e-01, -1.4884e-01, -9.3808e-02],\n",
      "          [-2.2845e-01, -7.4226e-02, -8.1514e-02, -2.7386e-01, -1.0956e-01],\n",
      "          [-1.2206e-01, -2.1743e-01, -2.9856e-01, -2.8731e-02,  9.6643e-02],\n",
      "          [-1.9837e-01, -3.8850e-02,  2.3305e-02,  4.9203e-02, -2.0349e-02]],\n",
      "\n",
      "         [[-5.4987e-02,  5.3640e-02,  1.3657e-01,  3.3789e-02,  1.3044e-02],\n",
      "          [ 2.0686e-01,  9.8982e-02,  2.2886e-02, -1.6888e-01, -1.4014e-01],\n",
      "          [-5.6482e-02, -3.4683e-01, -2.9879e-01, -2.3859e-02,  9.2410e-02],\n",
      "          [-2.4410e-01, -1.9612e-01,  1.5708e-02,  6.0461e-02,  1.5105e-02],\n",
      "          [-1.2848e-01, -2.5960e-02, -2.5405e-02,  4.4113e-02,  2.3974e-02]],\n",
      "\n",
      "         [[-1.9963e-01, -1.3140e-01, -1.2006e-01, -4.9717e-02, -5.7649e-02],\n",
      "          [-2.1639e-01, -5.4895e-03,  5.7837e-02, -3.0398e-02, -4.7915e-03],\n",
      "          [-7.1172e-02,  6.1029e-02,  4.3222e-02,  9.2570e-03, -3.3017e-02],\n",
      "          [ 1.1277e-01,  5.3487e-02,  2.5429e-02, -1.5615e-01, -6.8837e-02],\n",
      "          [-1.8141e-02, -7.9692e-02, -1.7832e-01,  6.7076e-03, -5.7966e-03]],\n",
      "\n",
      "         [[-3.4037e-03, -8.1525e-02, -9.1144e-02, -1.2012e-01, -3.8847e-02],\n",
      "          [-8.1585e-02, -3.4195e-01, -2.9969e-01, -1.5794e-01,  5.5974e-02],\n",
      "          [-2.7197e-01, -5.5564e-01, -2.2718e-01,  7.2226e-02,  2.0950e-01],\n",
      "          [-4.1645e-01, -1.7614e-01,  6.6150e-02,  1.8177e-01,  1.3976e-01],\n",
      "          [-4.0023e-01,  1.9657e-01,  1.3239e-01,  7.7586e-02, -4.8723e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.1817e-02,  2.2182e-02, -3.6101e-02, -1.8145e-03,  8.3493e-02],\n",
      "          [-3.4730e-02, -1.4041e-01, -2.8523e-02,  2.5410e-02, -3.5764e-02],\n",
      "          [-4.4214e-02, -5.3420e-02, -5.0477e-02, -1.3260e-01,  3.5931e-02],\n",
      "          [ 1.2647e-01, -1.1890e-01, -1.6404e-01, -5.2429e-02,  6.4005e-02],\n",
      "          [-9.0396e-02, -2.6148e-01, -2.2544e-01, -3.6708e-02, -9.4462e-02]],\n",
      "\n",
      "         [[ 3.6685e-02, -2.6414e-03, -2.8529e-02,  9.4394e-02,  9.4378e-02],\n",
      "          [-1.8894e-01, -1.8850e-02,  7.1730e-03,  8.9038e-02,  1.0774e-01],\n",
      "          [-1.7074e-01, -2.8408e-02,  5.8330e-02,  9.0931e-02,  3.0340e-02],\n",
      "          [-8.4409e-02, -1.5591e-02,  8.5304e-02,  6.4770e-02, -4.2567e-02],\n",
      "          [-1.0004e-01,  5.6348e-02,  8.6745e-02, -3.7125e-02, -8.0133e-02]],\n",
      "\n",
      "         [[-7.4653e-02, -9.8780e-02, -2.4018e-02,  1.1004e-01,  2.7725e-02],\n",
      "          [-4.2694e-02, -1.5853e-01, -2.7159e-02,  1.1494e-01,  4.2087e-02],\n",
      "          [-1.6365e-02, -1.0246e-01,  3.3577e-02,  1.2900e-01,  5.0675e-02],\n",
      "          [-4.7899e-03, -1.7037e-01,  1.1886e-01,  1.5151e-01,  1.7228e-02],\n",
      "          [ 1.1160e-03, -6.3331e-02,  1.8023e-01,  1.0539e-01,  1.1330e-01]],\n",
      "\n",
      "         [[ 4.5944e-02,  4.4470e-02,  2.1796e-02, -5.8597e-02, -1.1427e-01],\n",
      "          [ 1.4816e-01,  3.7034e-02,  3.0883e-02, -1.6358e-01, -3.9052e-02],\n",
      "          [ 1.1007e-01, -1.5775e-01, -1.4659e-01, -1.6719e-01, -5.1862e-02],\n",
      "          [ 1.4263e-01, -4.7980e-02, -1.1469e-01, -5.4084e-02, -5.5033e-02],\n",
      "          [ 1.4781e-01, -3.8136e-02, -1.3197e-01, -1.5932e-02,  7.7064e-02]],\n",
      "\n",
      "         [[ 8.2211e-02, -9.8334e-03,  1.3691e-02, -2.7329e-02,  8.6698e-03],\n",
      "          [ 4.3884e-02, -2.4471e-01, -7.5493e-02, -2.3862e-02, -8.8378e-02],\n",
      "          [-1.6970e-02, -1.2629e-01, -1.5732e-01, -4.2372e-02, -4.7084e-02],\n",
      "          [ 8.7678e-02, -1.1094e-01, -8.6247e-02,  1.8099e-02, -2.5063e-02],\n",
      "          [-1.5855e-01, -2.1026e-01, -4.2184e-02, -2.4227e-02, -1.6959e-03]],\n",
      "\n",
      "         [[-1.4986e-01, -2.2105e-01, -2.0040e-01,  8.5758e-03, -5.9773e-02],\n",
      "          [-4.0386e-02, -1.3959e-01, -1.5406e-01,  9.5148e-02,  8.7979e-02],\n",
      "          [ 4.8921e-02, -9.4910e-02, -8.3338e-02,  8.8777e-02, -4.2203e-02],\n",
      "          [ 5.4447e-02, -3.1439e-01, -9.3188e-02,  1.0831e-01,  1.2497e-02],\n",
      "          [-4.6453e-02, -1.1596e-01,  1.0696e-01,  6.4914e-02,  7.9434e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1636e-01, -1.4807e-02,  2.6952e-02, -8.0741e-03, -1.8210e-02],\n",
      "          [ 3.3446e-02, -1.2160e-01,  6.7170e-02,  4.8999e-02, -3.2640e-02],\n",
      "          [ 1.2554e-02,  3.9603e-02,  1.3323e-01,  3.5882e-02, -2.3900e-01],\n",
      "          [ 1.3308e-01,  3.8286e-02, -3.0315e-01, -2.2966e-01, -4.7806e-02],\n",
      "          [-1.1763e-01, -1.7678e-01,  1.8248e-02,  1.0734e-01,  1.6546e-03]],\n",
      "\n",
      "         [[ 3.0793e-02,  1.2822e-02,  3.0937e-02,  2.7713e-02,  2.8521e-02],\n",
      "          [-8.3878e-02, -4.1991e-02,  9.7274e-02,  5.1837e-02,  2.5978e-02],\n",
      "          [-7.2717e-02,  6.2968e-02,  1.0666e-01, -2.8524e-02, -1.7641e-01],\n",
      "          [-9.2010e-04,  1.9077e-02,  3.6390e-02, -8.1079e-02, -1.1454e-01],\n",
      "          [ 6.4285e-02, -1.1903e-02, -1.0219e-01, -8.8348e-02, -4.5178e-02]],\n",
      "\n",
      "         [[ 2.8882e-01,  1.4271e-01,  6.2512e-02,  4.4021e-02, -6.8442e-02],\n",
      "          [ 1.2968e-01, -8.2726e-02, -1.0338e-01, -7.5168e-02,  7.5708e-02],\n",
      "          [ 1.3412e-01, -2.2101e-01, -1.4731e-01, -2.2639e-02,  2.0970e-01],\n",
      "          [-6.1231e-02, -3.1684e-01, -3.1143e-02,  2.2321e-01,  6.5525e-02],\n",
      "          [ 7.9710e-02,  1.5542e-01,  4.8575e-02,  1.0912e-01, -2.5945e-02]],\n",
      "\n",
      "         [[ 6.4279e-02,  1.7301e-01,  1.3702e-01, -1.2245e-01, -2.9884e-01],\n",
      "          [ 6.3229e-02, -1.6894e-01, -1.6978e-01,  6.8446e-03, -3.9879e-02],\n",
      "          [ 3.8385e-02, -2.7143e-01, -6.3426e-02, -1.7608e-02,  1.8723e-02],\n",
      "          [ 3.6839e-02,  2.1722e-02, -1.3342e-02,  9.3010e-02, -7.1119e-02],\n",
      "          [ 1.6776e-01,  1.0099e-01,  1.7348e-01, -6.5027e-02, -1.4565e-01]],\n",
      "\n",
      "         [[ 1.4052e-01, -4.1154e-02, -7.7938e-03,  3.6774e-02,  4.2250e-02],\n",
      "          [-5.3324e-02,  5.2313e-02, -1.3824e-01, -9.6189e-02,  2.8658e-02],\n",
      "          [-9.8213e-02, -8.8065e-02, -6.2526e-02, -2.3904e-02, -1.4747e-01],\n",
      "          [-1.0626e-01, -1.1373e-01, -2.9279e-01, -6.3807e-02, -2.1192e-02],\n",
      "          [-1.3273e-01,  5.4201e-02,  1.1191e-01,  1.0241e-01,  7.3780e-02]],\n",
      "\n",
      "         [[-3.8328e-02, -4.5623e-02, -1.5402e-01, -8.9624e-02,  5.7353e-02],\n",
      "          [-8.7370e-02, -2.1898e-01, -3.2908e-02, -7.3517e-02,  1.1625e-01],\n",
      "          [-2.6615e-01, -2.1329e-01, -4.5815e-02,  3.1702e-02,  6.3152e-03],\n",
      "          [-2.8318e-01, -5.1009e-02,  1.9771e-01,  1.1567e-01, -1.5439e-01],\n",
      "          [ 3.6166e-02,  1.2138e-01,  8.1920e-02, -8.7783e-02, -5.4177e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.9934e-01,  1.1872e-01, -5.6125e-02,  3.8922e-02,  2.8421e-03],\n",
      "          [ 8.3867e-02,  6.5188e-02, -5.9422e-03, -4.5005e-02, -1.3209e-01],\n",
      "          [-8.0760e-02,  9.5649e-02, -1.1778e-02, -4.6405e-02,  4.6141e-02],\n",
      "          [-3.1948e-02, -9.2163e-02, -2.2152e-02,  1.5716e-01,  7.9658e-02],\n",
      "          [-7.3042e-02, -8.2970e-02, -2.3561e-01, -1.4449e-01, -1.5999e-01]],\n",
      "\n",
      "         [[ 5.9830e-02,  8.5773e-03, -9.1544e-02, -1.3011e-01, -5.5282e-02],\n",
      "          [ 1.5461e-01,  9.9889e-02, -7.6823e-03, -3.5979e-02,  3.2579e-03],\n",
      "          [ 1.1054e-01,  6.3648e-02,  1.3227e-02,  3.2628e-03,  1.3038e-02],\n",
      "          [ 1.1453e-01, -2.5743e-02,  2.9985e-02,  2.0416e-02, -6.3056e-02],\n",
      "          [ 7.2743e-02,  3.8109e-02, -8.2419e-02, -9.3970e-02, -3.6060e-02]],\n",
      "\n",
      "         [[ 3.3100e-02, -7.5973e-02, -4.3694e-02, -6.0177e-02,  4.2262e-02],\n",
      "          [-1.4073e-01, -9.3016e-02,  3.0364e-02, -6.2557e-02, -8.1103e-02],\n",
      "          [ 2.3142e-01, -8.6406e-03,  1.3738e-01,  2.3915e-01,  1.8543e-01],\n",
      "          [ 1.4777e-01,  2.9260e-02,  1.5096e-01, -3.5418e-02,  1.0650e-01],\n",
      "          [-1.6591e-01, -7.8815e-02, -1.5932e-01, -2.0103e-01, -2.0696e-01]],\n",
      "\n",
      "         [[-2.6677e-01, -2.0797e-01, -6.6036e-02,  4.8612e-02, -7.1016e-02],\n",
      "          [-1.5859e-01, -2.0752e-01, -2.7433e-01, -1.7161e-01, -7.8741e-02],\n",
      "          [-4.2561e-02, -4.6498e-03, -3.5566e-02, -1.2942e-01, -1.8206e-03],\n",
      "          [ 5.0993e-02,  4.4298e-02,  5.4886e-02,  1.0319e-01,  1.6713e-01],\n",
      "          [ 1.4023e-01,  1.7886e-01,  8.0587e-02,  5.5260e-02,  1.6211e-01]],\n",
      "\n",
      "         [[-5.8785e-03,  3.1889e-02, -1.3903e-02,  6.9898e-02,  5.1467e-02],\n",
      "          [-9.0041e-02,  2.0166e-02,  3.8667e-02,  5.1026e-02, -3.0391e-02],\n",
      "          [ 1.1111e-01,  6.9041e-04,  5.0474e-02,  1.5481e-01,  1.4264e-01],\n",
      "          [ 8.2160e-02,  5.9732e-02,  5.8461e-02,  5.0226e-02,  1.4293e-01],\n",
      "          [-2.2125e-02, -1.3262e-01, -1.7154e-01, -2.5744e-01, -2.5812e-01]],\n",
      "\n",
      "         [[-3.2738e-02, -6.3184e-02, -4.4475e-03,  4.4207e-02, -6.3966e-02],\n",
      "          [-1.4028e-01, -8.2750e-02, -3.9325e-02,  5.9921e-02, -1.2817e-01],\n",
      "          [ 2.8782e-02, -1.3846e-01, -1.0279e-01, -1.3548e-03, -1.3071e-02],\n",
      "          [-7.9671e-03, -2.1391e-01, -3.8766e-02, -1.8922e-01, -1.6469e-01],\n",
      "          [-2.0629e-01, -1.3051e-01, -1.2044e-01, -2.8110e-01, -2.1358e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.3693e-01, -2.8577e-01, -1.3072e-01,  2.6289e-02,  9.7855e-02],\n",
      "          [-3.8774e-01, -2.2088e-01, -2.3079e-01, -4.6860e-02,  4.9030e-02],\n",
      "          [-1.7932e-02,  1.9414e-02, -2.0000e-01, -3.2197e-01, -4.5903e-01],\n",
      "          [ 1.7051e-01,  3.9387e-02,  7.9619e-02, -4.6262e-03,  6.0185e-02],\n",
      "          [ 2.4264e-01,  1.9118e-01,  2.1695e-01,  1.9301e-01,  1.5897e-01]],\n",
      "\n",
      "         [[ 3.1937e-02, -1.6071e-01, -2.1984e-01,  4.6856e-03,  1.1302e-01],\n",
      "          [-1.9798e-02, -1.9956e-01, -2.3506e-01, -4.2783e-02,  1.2290e-01],\n",
      "          [-1.2996e-01, -2.8244e-01, -2.9798e-01, -1.7192e-01, -1.0160e-01],\n",
      "          [-4.7330e-02, -5.1290e-02, -6.6556e-02, -1.0431e-01, -8.8221e-02],\n",
      "          [-5.6919e-02,  1.6908e-03,  1.6963e-02, -6.6924e-03, -5.0672e-02]],\n",
      "\n",
      "         [[ 1.2896e-02, -1.8406e-02, -2.6817e-01, -9.4896e-02,  1.1987e-01],\n",
      "          [-1.2953e-01, -1.2704e-01, -1.1385e-01, -2.5176e-02, -1.3131e-02],\n",
      "          [-3.4969e-03, -1.5205e-01, -1.7888e-02, -1.7984e-01, -2.1812e-01],\n",
      "          [-8.6095e-03, -8.1716e-02,  3.7569e-02, -5.1104e-02,  4.5796e-02],\n",
      "          [ 2.5094e-02,  9.2173e-02,  6.4341e-02,  8.5285e-02,  2.1581e-01]],\n",
      "\n",
      "         [[-1.8123e-01, -1.1069e-01, -2.7173e-02, -6.6448e-03, -2.4092e-02],\n",
      "          [-2.9975e-02, -3.3132e-02, -1.5520e-01,  1.3898e-02,  2.1749e-01],\n",
      "          [-1.7404e-01, -2.5471e-01,  4.5244e-03,  1.4186e-01,  2.2512e-01],\n",
      "          [-1.0060e-01, -4.7001e-02,  4.7477e-02, -2.4074e-01, -2.9071e-01],\n",
      "          [ 1.1929e-01,  1.4430e-01,  1.1416e-01,  2.2218e-01,  1.0955e-01]],\n",
      "\n",
      "         [[-2.1664e-01, -1.6820e-01, -2.5026e-01, -2.8174e-02,  4.6455e-02],\n",
      "          [-1.8052e-01, -2.1744e-01, -1.7505e-01, -1.0519e-01,  8.4114e-03],\n",
      "          [ 1.0093e-02, -3.8194e-02, -6.8164e-02, -1.4971e-01, -4.0881e-01],\n",
      "          [ 1.0723e-01,  8.5843e-02,  1.8405e-01,  1.4348e-01,  1.5657e-01],\n",
      "          [ 1.9175e-01,  2.4992e-01,  1.2366e-01,  1.8463e-01,  2.3124e-01]],\n",
      "\n",
      "         [[ 7.1289e-02,  1.8162e-01,  1.8482e-01, -8.1548e-02,  8.1957e-02],\n",
      "          [ 7.6445e-02,  4.6406e-02, -1.6294e-01, -1.1568e-01,  8.6175e-02],\n",
      "          [-8.6301e-02, -5.3229e-02, -1.6901e-01, -2.6375e-01, -1.2208e-01],\n",
      "          [-6.4894e-02, -8.1385e-02, -9.7302e-02, -1.8133e-01, -7.8711e-02],\n",
      "          [-1.4550e-01,  5.0139e-02,  1.4462e-01,  7.3656e-02,  4.4683e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3622e-01,  9.0478e-02,  9.4849e-02,  4.1034e-02,  3.3325e-02],\n",
      "          [ 4.7223e-03, -6.3485e-02,  9.6528e-03, -1.0369e-01, -2.7881e-02],\n",
      "          [-1.7053e-01, -9.9697e-02, -2.2703e-01, -2.1449e-01, -2.1218e-01],\n",
      "          [-8.6566e-02, -1.3382e-01,  5.7969e-02,  6.5813e-03, -3.0012e-02],\n",
      "          [-1.2700e-01, -7.1751e-02,  5.4042e-02, -6.4184e-02, -1.2229e-01]],\n",
      "\n",
      "         [[ 1.4184e-02,  4.1948e-02,  5.6093e-02, -8.4238e-03, -1.9141e-02],\n",
      "          [-2.4984e-02,  2.7053e-02,  3.8146e-02,  6.9088e-02, -6.7334e-02],\n",
      "          [ 3.0613e-02,  1.2451e-01,  9.6231e-02, -6.6629e-02, -1.7910e-01],\n",
      "          [ 2.5360e-02,  1.0426e-01,  1.1702e-01, -1.4391e-02, -1.8657e-01],\n",
      "          [ 1.9230e-02,  6.1585e-02,  4.5945e-02, -1.4951e-02, -2.4175e-01]],\n",
      "\n",
      "         [[ 8.6013e-02, -8.2724e-02, -1.4903e-02, -2.9206e-03,  1.2421e-01],\n",
      "          [-7.2224e-02, -1.8976e-02,  3.2297e-02, -1.5099e-01, -3.7495e-02],\n",
      "          [-1.9520e-01,  9.4077e-02,  9.4906e-02,  5.7729e-02, -1.0780e-01],\n",
      "          [-8.7868e-02,  4.8361e-02,  1.3417e-01,  9.9120e-02,  3.5835e-02],\n",
      "          [-1.0232e-01,  4.4692e-02,  7.7318e-02,  1.5576e-01,  1.0308e-02]],\n",
      "\n",
      "         [[-9.2519e-02, -3.6665e-02, -2.6830e-02,  2.4303e-02,  1.2399e-01],\n",
      "          [-3.3003e-03, -8.6299e-02, -5.0139e-02, -9.3133e-03,  4.9304e-02],\n",
      "          [-1.6550e-01, -9.0648e-02, -1.9158e-01, -1.0904e-01,  1.0656e-01],\n",
      "          [-3.0480e-01, -2.3483e-01, -9.5482e-02, -7.7337e-02,  1.1000e-01],\n",
      "          [ 3.1605e-04,  1.0325e-01,  1.0771e-01,  6.3359e-02,  6.1204e-02]],\n",
      "\n",
      "         [[ 1.2746e-01,  1.2747e-01,  2.4850e-02, -4.5810e-02, -3.1085e-02],\n",
      "          [-1.7465e-01, -3.8326e-02, -9.6880e-02, -7.1652e-02, -1.7007e-01],\n",
      "          [-2.1799e-01, -1.0844e-01,  2.2380e-02, -4.3690e-02, -1.1763e-01],\n",
      "          [-1.6960e-01, -3.1648e-02, -1.1335e-02,  8.0002e-02,  6.6755e-02],\n",
      "          [-2.4392e-01, -9.1662e-02, -6.1540e-02,  4.0829e-03,  7.2498e-02]],\n",
      "\n",
      "         [[-1.1129e-01, -1.3493e-01, -1.4691e-01, -1.5220e-01, -5.0920e-02],\n",
      "          [-2.4299e-01, -1.8988e-01,  1.2245e-01,  5.0027e-02, -1.4723e-01],\n",
      "          [-3.8820e-01,  3.9004e-02,  7.1773e-02,  1.1543e-01, -2.1122e-01],\n",
      "          [-1.4856e-01,  1.1940e-01,  1.0239e-01,  1.8030e-02, -9.9282e-02],\n",
      "          [-7.7230e-02,  1.5329e-02, -8.4130e-02,  8.7320e-02, -7.3805e-02]]]],\n",
      "       requires_grad=True)\n",
      "fc1.bias Parameter containing:\n",
      "tensor([-0.0787, -0.0700,  0.0285, -0.0533, -0.0217, -0.0027, -0.0127, -0.0022,\n",
      "         0.0036,  0.0280,  0.0313,  0.0112, -0.0007, -0.0098, -0.0494, -0.0960,\n",
      "         0.0101,  0.0168, -0.0584,  0.0191, -0.0053,  0.0396,  0.0717,  0.0661,\n",
      "         0.0784, -0.0778, -0.0327,  0.1194, -0.0497,  0.0785,  0.0683,  0.0309,\n",
      "         0.0232,  0.0203,  0.0305, -0.0035,  0.0127, -0.0081, -0.0886,  0.0376,\n",
      "         0.0642,  0.0355, -0.0104,  0.0901, -0.0238,  0.0307,  0.0646, -0.0537,\n",
      "         0.0385,  0.0774,  0.0074, -0.0680, -0.0359, -0.0320, -0.0223,  0.0082,\n",
      "         0.0048, -0.0249, -0.0344,  0.0692,  0.0206, -0.0677,  0.0702, -0.0742,\n",
      "        -0.0109, -0.0761, -0.0338,  0.0120, -0.0293,  0.0230,  0.1051,  0.0121,\n",
      "        -0.0592, -0.0236, -0.0579, -0.0027, -0.0653, -0.0620, -0.0392, -0.0327,\n",
      "        -0.0815,  0.0489, -0.0553,  0.0379,  0.0327,  0.0852, -0.0408, -0.0409,\n",
      "        -0.0220, -0.0054,  0.0286, -0.0711, -0.0169,  0.0911, -0.0249,  0.0391,\n",
      "         0.0217,  0.1545,  0.0185, -0.0233, -0.0528, -0.0210,  0.0194,  0.0249,\n",
      "         0.0042,  0.0638, -0.0701,  0.0632,  0.0459,  0.0438, -0.0505,  0.0564,\n",
      "         0.0075, -0.0409,  0.0476, -0.0514, -0.0959, -0.0315, -0.0325, -0.0325],\n",
      "       requires_grad=True)\n",
      "fc1.weight_orig Parameter containing:\n",
      "tensor([[-1.4931e-01, -8.1400e-02, -2.8217e-02,  ..., -8.5749e-02,\n",
      "          4.7628e-03, -1.4836e-02],\n",
      "        [ 3.6634e-02, -9.0928e-02, -1.6190e-01,  ...,  1.1014e-04,\n",
      "          1.7323e-02, -1.2542e-01],\n",
      "        [-5.8885e-02, -2.1049e-02, -4.5248e-02,  ..., -2.9316e-02,\n",
      "         -7.6377e-02, -1.3025e-02],\n",
      "        ...,\n",
      "        [-5.3593e-02, -3.8646e-02, -7.9696e-03,  ...,  2.8406e-02,\n",
      "         -3.0673e-02,  3.3078e-02],\n",
      "        [ 1.1528e-02,  3.2270e-03, -2.5051e-02,  ..., -7.8192e-02,\n",
      "         -2.9738e-02, -6.0912e-02],\n",
      "        [ 2.7393e-03, -7.5861e-02, -2.4266e-02,  ..., -6.6741e-02,\n",
      "         -5.2308e-02, -6.0967e-02]], requires_grad=True)\n",
      "fc2.bias Parameter containing:\n",
      "tensor([-0.0979, -0.0702,  0.0187, -0.1648, -0.0910,  0.1451, -0.0210,  0.0153,\n",
      "        -0.1186, -0.0883,  0.0238,  0.0889, -0.0073, -0.0908,  0.0328, -0.0117,\n",
      "         0.0799,  0.1011, -0.0243,  0.0093,  0.1880,  0.0492, -0.0900,  0.1067,\n",
      "        -0.0175, -0.0354,  0.0652,  0.0080,  0.1353,  0.0890, -0.0984, -0.0243,\n",
      "         0.0793,  0.0077, -0.0976, -0.0592,  0.1050,  0.0004, -0.0637,  0.1579,\n",
      "        -0.0830, -0.1260,  0.1181, -0.0782, -0.0915, -0.1003,  0.0283, -0.0923,\n",
      "         0.0267, -0.0349, -0.0566,  0.0124, -0.1365, -0.0693,  0.0284,  0.0413,\n",
      "         0.0608,  0.0465,  0.0714, -0.0087,  0.0359, -0.0698, -0.0963, -0.0322,\n",
      "         0.0109,  0.1183,  0.0834,  0.1054,  0.0690,  0.0341,  0.0562, -0.0427,\n",
      "        -0.0460, -0.1075, -0.0781,  0.0422, -0.0255,  0.1313, -0.0923, -0.0667,\n",
      "        -0.0641,  0.0260,  0.1377, -0.0153], requires_grad=True)\n",
      "fc2.weight_orig Parameter containing:\n",
      "tensor([[-0.0403,  0.0189, -0.0504,  ...,  0.0355, -0.0393,  0.0306],\n",
      "        [-0.1060,  0.0996,  0.0195,  ..., -0.2005, -0.2178,  0.0542],\n",
      "        [ 0.0077, -0.0396,  0.1304,  ...,  0.0028, -0.0136,  0.0092],\n",
      "        ...,\n",
      "        [ 0.1185, -0.1087, -0.1204,  ...,  0.1313, -0.0172,  0.0289],\n",
      "        [ 0.1246, -0.2040,  0.0757,  ...,  0.0253, -0.0295,  0.0384],\n",
      "        [-0.0465, -0.0322,  0.0339,  ...,  0.0984, -0.0506,  0.0956]],\n",
      "       requires_grad=True)\n",
      "fc3.bias Parameter containing:\n",
      "tensor([-0.0679, -0.0114, -0.1257, -0.0408, -0.0365,  0.0168, -0.1293,  0.0521,\n",
      "         0.1616,  0.0352], requires_grad=True)\n",
      "fc3.weight_orig Parameter containing:\n",
      "tensor([[ 6.1784e-02, -5.2693e-02, -1.2049e-02, -5.0566e-02, -1.6151e-01,\n",
      "          1.2307e-01, -5.8265e-02,  5.5493e-02, -6.3695e-02,  4.6561e-02,\n",
      "         -1.1254e-01, -2.8722e-01, -1.9957e-01, -7.4827e-02, -1.4039e-01,\n",
      "          2.2361e-02, -1.3699e-01,  1.3495e-01,  5.0341e-02,  1.2965e-01,\n",
      "         -1.9658e-01,  9.8986e-02,  1.2190e-01,  9.5667e-02, -3.0755e-01,\n",
      "         -4.0649e-02, -8.7970e-02, -1.7660e-01, -1.0960e-01, -6.3908e-02,\n",
      "         -7.1279e-02,  5.0142e-02,  7.0685e-02,  9.0679e-02, -1.5233e-01,\n",
      "          1.2752e-01,  7.1976e-02,  1.5574e-01, -1.2146e-01,  1.2406e-01,\n",
      "         -1.2838e-01,  2.8144e-02, -6.9473e-03, -2.0533e-01, -7.4322e-02,\n",
      "          2.0985e-01,  8.3540e-02, -9.8131e-02, -9.5179e-02, -7.5556e-02,\n",
      "         -1.2714e-01,  1.7885e-02, -1.7260e-02,  9.3853e-02,  2.2810e-02,\n",
      "          3.9251e-02,  1.0808e-02,  3.4856e-02, -9.5253e-02,  1.0390e-01,\n",
      "         -1.3664e-01,  4.8868e-02, -2.5188e-01, -9.1593e-02, -9.7316e-03,\n",
      "         -7.6411e-02,  9.2178e-03,  1.0471e-01, -1.5612e-01, -2.3868e-02,\n",
      "          1.3795e-01, -1.3289e-01, -9.3607e-02, -9.3055e-02, -4.8903e-02,\n",
      "         -9.9632e-02, -4.5830e-02,  1.1985e-01, -1.8550e-02,  5.2886e-02,\n",
      "          1.1137e-01, -2.3623e-02, -6.2373e-02, -1.7604e-01],\n",
      "        [-5.8877e-02, -1.9022e-01,  1.0398e-01,  8.3659e-02, -1.1719e-01,\n",
      "         -2.8178e-01, -1.8867e-01, -1.9436e-01, -6.7900e-02,  2.6826e-02,\n",
      "          9.6801e-02,  1.4587e-01,  5.9495e-02,  5.7057e-02, -6.8606e-02,\n",
      "         -4.6710e-02,  1.8275e-02,  6.7365e-02, -2.4231e-01, -1.6468e-01,\n",
      "         -1.4168e-01,  1.8260e-03, -1.6125e-01, -1.1466e-01,  3.5727e-02,\n",
      "         -1.7439e-02, -1.5176e-01,  4.5442e-02,  3.0313e-02, -1.1511e-02,\n",
      "          4.2790e-02, -2.1341e-02, -1.1722e-01, -8.4147e-02, -2.0274e-01,\n",
      "          1.5434e-01,  6.4849e-02, -9.4971e-02, -6.6055e-02, -1.1736e-01,\n",
      "          1.4885e-01, -8.5724e-02, -1.6746e-01,  4.8859e-02,  1.7982e-01,\n",
      "          3.0482e-02,  1.5816e-01,  7.2136e-02,  1.0569e-01,  1.4066e-01,\n",
      "         -2.3646e-01, -2.4549e-01, -2.1465e-01,  5.7705e-04, -1.6565e-01,\n",
      "          5.6789e-02, -3.6594e-02,  1.2096e-01,  2.9995e-02,  5.6652e-02,\n",
      "          3.8426e-02, -2.2711e-01,  1.5287e-01,  6.5413e-02, -1.0162e-02,\n",
      "          2.6233e-02,  5.5234e-02,  1.0463e-01, -1.5300e-01, -1.4713e-01,\n",
      "          1.2383e-01, -2.2658e-01, -5.9503e-02, -1.8008e-02, -5.2163e-02,\n",
      "         -1.7903e-01, -6.2931e-02, -2.1832e-02, -2.3798e-01,  3.0054e-02,\n",
      "          1.4623e-01, -1.1267e-01,  8.4995e-02,  1.0474e-01],\n",
      "        [-1.4880e-01,  1.9091e-01,  5.4108e-02, -1.1780e-01, -1.3966e-01,\n",
      "         -9.7270e-02, -4.0223e-02,  4.0309e-02,  8.2314e-03, -1.0140e-01,\n",
      "          1.8684e-01, -4.2373e-02,  5.2824e-02, -3.7266e-02, -2.0021e-01,\n",
      "          4.1143e-02,  1.1855e-01,  2.0131e-02, -5.9277e-02, -1.2593e-01,\n",
      "         -1.4514e-01, -1.2835e-01,  2.7380e-02,  8.4057e-02,  1.5840e-01,\n",
      "         -1.5036e-01,  1.4720e-03, -4.4854e-02, -7.6273e-02,  1.0725e-01,\n",
      "          1.4293e-01, -7.4916e-02,  7.9875e-02, -5.4510e-02,  5.3445e-02,\n",
      "         -2.9074e-02, -6.9242e-02,  6.7642e-02,  9.1114e-02,  9.1416e-02,\n",
      "         -9.7416e-02, -6.7271e-02,  7.4851e-02, -1.6129e-01, -9.8965e-02,\n",
      "         -2.6546e-01, -2.0600e-02,  3.4941e-04, -2.9977e-01, -4.3605e-02,\n",
      "          2.6923e-02,  1.3163e-01,  1.7294e-02,  8.3416e-02,  1.3006e-01,\n",
      "          1.1417e-01, -2.2490e-01, -3.0217e-02,  1.3303e-01,  8.4261e-02,\n",
      "          5.9271e-03, -1.5399e-03, -2.1604e-01, -1.8721e-01,  1.3262e-01,\n",
      "         -9.1745e-02, -1.2409e-04,  1.5374e-01,  5.6421e-02, -1.8828e-01,\n",
      "         -3.4124e-01, -1.2815e-01,  1.6308e-02,  6.0656e-02, -4.8711e-02,\n",
      "          8.5832e-02, -3.1802e-01, -9.0612e-02, -4.5216e-02,  1.2457e-01,\n",
      "         -9.7786e-03,  1.6670e-01, -2.2775e-01, -7.3508e-02],\n",
      "        [-1.2358e-01, -4.6660e-02, -1.9324e-01,  1.5585e-02,  4.1933e-02,\n",
      "          1.6525e-01, -1.1360e-01,  2.9737e-02,  6.6180e-02, -1.1713e-01,\n",
      "         -2.1944e-01, -1.2180e-01, -9.4423e-02,  1.9580e-02,  8.0801e-02,\n",
      "         -8.0011e-02,  1.4704e-01, -2.1414e-01, -1.5608e-01, -6.5718e-02,\n",
      "          1.7403e-01,  5.1514e-02, -1.6048e-02,  3.1593e-02,  7.7626e-02,\n",
      "         -1.3084e-01, -1.3629e-01,  7.3243e-02,  1.9736e-01,  1.1905e-02,\n",
      "          4.8646e-03, -1.7500e-02,  6.5176e-03,  7.0919e-03, -1.5684e-01,\n",
      "         -9.0493e-02,  1.0511e-01, -1.7318e-01,  8.4860e-02,  1.0811e-01,\n",
      "          3.1463e-02, -1.8963e-03, -7.3599e-02,  4.1864e-02, -9.5080e-02,\n",
      "         -6.8878e-03, -3.1896e-01,  3.0367e-02,  9.5422e-02, -1.1669e-01,\n",
      "         -7.6480e-02,  7.0485e-03, -1.4698e-01, -4.3549e-02,  4.7541e-02,\n",
      "          1.1954e-01,  1.3212e-01,  2.6624e-02, -6.1053e-02, -1.6907e-01,\n",
      "         -8.5527e-02, -2.9269e-01,  7.6069e-02, -1.1723e-01,  7.2713e-02,\n",
      "         -2.1483e-01,  7.1400e-02,  1.7842e-02,  1.5443e-01,  3.6573e-02,\n",
      "         -6.8239e-03, -1.7213e-01, -7.6505e-02, -1.7754e-02,  6.9904e-02,\n",
      "         -3.1058e-02, -1.7269e-02,  9.4065e-02, -9.3228e-02,  8.5303e-02,\n",
      "         -1.0485e-01,  9.3467e-04,  9.1692e-02, -1.8099e-01],\n",
      "        [ 1.7837e-02,  4.8808e-02,  2.6058e-02, -4.8830e-02,  1.2222e-02,\n",
      "         -5.4164e-02, -2.4231e-01,  1.2236e-01,  1.0065e-01, -6.2055e-02,\n",
      "         -1.2311e-01,  1.4971e-02, -8.1029e-02,  1.1538e-01, -5.3531e-02,\n",
      "         -5.9074e-01, -2.0970e-02, -1.6089e-01,  1.0206e-01,  1.1502e-01,\n",
      "          3.3839e-02, -1.3696e-01, -6.6223e-02,  1.0719e-02, -1.1276e-01,\n",
      "          1.7149e-01, -8.1340e-02, -1.3557e-02, -1.3874e-01, -1.0746e-01,\n",
      "         -8.1673e-02,  1.2765e-02,  1.9815e-02, -1.0959e-01, -2.2022e-01,\n",
      "         -6.0934e-02, -2.7868e-01,  2.7586e-04,  6.8522e-02, -2.3463e-01,\n",
      "         -4.6312e-02, -5.8710e-02,  7.4423e-02,  1.6333e-01, -1.6482e-01,\n",
      "         -3.2136e-02, -1.1887e-02,  3.0037e-02,  3.8218e-02,  1.9517e-01,\n",
      "          1.0131e-01,  1.4093e-01,  7.6865e-02,  1.3276e-01, -1.1075e-01,\n",
      "         -1.6719e-01,  6.2778e-04, -4.9109e-02,  7.4669e-02, -1.4949e-01,\n",
      "          9.8632e-02, -4.1769e-02, -4.0798e-02, -1.0797e-01,  5.2900e-02,\n",
      "         -5.2859e-02,  4.4742e-02, -1.3577e-01, -1.6165e-01,  4.7431e-02,\n",
      "         -6.4072e-02,  1.6687e-01,  7.1076e-02, -3.1720e-01,  9.2313e-02,\n",
      "          1.2011e-01, -1.0924e-01, -5.2347e-02, -7.6645e-02, -3.7323e-02,\n",
      "          2.3877e-02,  9.1719e-03, -5.6352e-02,  8.3521e-02],\n",
      "        [-2.4730e-02,  1.0574e-02, -1.1177e-01, -5.1889e-02, -2.2952e-01,\n",
      "          3.6999e-02, -8.1412e-02,  5.1793e-02,  8.3113e-02,  3.5599e-02,\n",
      "          6.7694e-02, -3.0271e-02,  6.6898e-02, -8.0274e-02,  1.7882e-01,\n",
      "         -2.7020e-01, -1.6865e-01, -1.5365e-01, -1.9916e-01, -1.4169e-01,\n",
      "          1.5195e-01, -5.1047e-02, -1.9179e-02,  7.3888e-02, -9.6034e-04,\n",
      "         -8.4378e-02,  7.8969e-02, -8.7762e-02, -1.6520e-01,  1.1319e-01,\n",
      "         -3.4625e-02, -7.8440e-02,  2.1482e-02,  1.0894e-01, -1.7354e-01,\n",
      "         -1.1877e-01, -4.1337e-03,  7.6970e-02, -1.4854e-01,  1.4784e-01,\n",
      "         -1.0619e-01, -1.0553e-01,  1.1761e-01, -1.0679e-01, -1.0383e-01,\n",
      "         -9.3088e-02, -6.2286e-03,  1.4951e-01,  9.2125e-02, -2.0155e-01,\n",
      "         -1.3801e-01,  6.8238e-02, -1.8055e-01, -1.8055e-01, -2.1921e-02,\n",
      "          9.6687e-02,  9.0634e-03,  7.8377e-02, -2.8313e-01, -9.5210e-02,\n",
      "         -6.4442e-02,  7.8980e-03,  3.1419e-02,  9.4333e-02,  1.2247e-02,\n",
      "          5.1171e-02, -1.7049e-01, -1.7002e-01, -3.4029e-02, -2.0593e-02,\n",
      "         -8.8224e-02,  4.0024e-02,  7.1731e-02, -6.0094e-03, -1.2941e-01,\n",
      "         -1.0907e-01,  2.1056e-02,  3.4591e-03, -1.4987e-01, -3.2359e-02,\n",
      "          1.6144e-01,  1.1489e-01,  4.6724e-02, -1.0429e-01],\n",
      "        [-1.1355e-01, -1.2095e-01,  6.3326e-02, -1.5069e-01, -6.5024e-02,\n",
      "          9.8420e-02,  7.6993e-02,  1.2428e-01, -6.0156e-03, -1.0346e-01,\n",
      "          1.0529e-01,  7.6487e-02,  1.4057e-02,  1.0477e-01,  8.0256e-02,\n",
      "         -2.1266e-01,  2.0323e-02, -8.4623e-04, -1.1988e-01,  1.3821e-01,\n",
      "         -9.7138e-02, -1.0255e-01,  2.1184e-02, -5.8038e-02, -3.0338e-02,\n",
      "          3.7413e-02, -3.7319e-02, -1.8798e-01,  2.8021e-02,  7.6640e-03,\n",
      "          2.1502e-01,  3.7065e-02,  3.9832e-02,  1.0810e-01, -2.4929e-01,\n",
      "         -1.0594e-01,  7.8233e-02, -3.7822e-02, -8.0051e-02, -9.8802e-02,\n",
      "         -1.5099e-02, -4.5327e-02,  1.0538e-01, -3.0897e-02,  2.7701e-02,\n",
      "         -2.6371e-02,  5.6803e-03, -1.5594e-01, -1.5291e-01, -9.4561e-02,\n",
      "          9.4909e-02, -1.2660e-01, -9.3379e-02, -1.7412e-02, -3.1907e-03,\n",
      "         -7.9937e-02, -3.6135e-02, -1.1179e-01, -2.1791e-01,  5.6886e-02,\n",
      "         -2.6478e-01,  1.6858e-01, -8.4939e-02,  8.7561e-02,  5.6886e-02,\n",
      "          4.6128e-02, -1.8762e-01, -2.0081e-01, -2.6743e-01,  7.0131e-02,\n",
      "          1.1936e-01, -2.4470e-01, -1.3742e-01, -1.6010e-01, -7.2238e-02,\n",
      "          1.2744e-01,  1.9313e-01, -2.0911e-01, -1.0179e-02,  3.8115e-02,\n",
      "          7.1875e-02, -7.0302e-02,  1.4931e-01, -1.7494e-01],\n",
      "        [-9.8809e-02, -7.5709e-02,  9.1106e-02,  5.7645e-02,  1.7974e-01,\n",
      "         -7.3412e-02,  9.2214e-02, -1.8264e-01, -4.0757e-02, -8.7676e-02,\n",
      "         -2.2731e-01,  1.9179e-02, -2.2363e-01, -1.4566e-01, -1.7624e-01,\n",
      "         -4.3029e-02, -5.5882e-02,  3.8010e-02, -1.8137e-02, -1.4039e-01,\n",
      "          5.9978e-02,  1.4209e-02,  1.6131e-02,  6.7937e-02,  6.7835e-02,\n",
      "         -3.0131e-01,  1.0712e-01,  4.4593e-02, -3.7173e-02, -3.8515e-03,\n",
      "         -1.9421e-01, -1.2546e-02, -1.4190e-01, -2.7793e-01,  8.8221e-02,\n",
      "         -1.5975e-02, -2.0679e-01,  2.6228e-02,  1.5248e-01, -4.3929e-02,\n",
      "         -1.0288e-01, -4.6996e-02, -1.9200e-01, -1.2589e-01, -2.3771e-01,\n",
      "         -8.3847e-02,  1.3947e-03, -1.0374e-01,  2.5764e-02, -3.1703e-02,\n",
      "          9.0211e-02, -4.5391e-02, -1.7376e-01,  1.8807e-02, -1.7157e-02,\n",
      "          4.5767e-02, -1.5103e-01,  8.4073e-02,  1.3585e-01, -2.7533e-01,\n",
      "          5.1122e-02, -9.2602e-02,  1.7328e-01, -1.5550e-01, -2.9951e-02,\n",
      "         -1.3311e-01, -2.0681e-01,  1.4413e-01, -4.4457e-02,  2.0641e-02,\n",
      "         -8.3034e-02, -9.3444e-02,  1.6699e-01,  3.8532e-02, -2.3614e-02,\n",
      "         -1.6725e-01, -4.9593e-02,  1.2122e-01,  3.5376e-02, -2.0680e-02,\n",
      "          1.0058e-01, -7.2913e-02,  3.4832e-02,  1.5193e-01],\n",
      "        [ 1.3947e-02, -2.7895e-01,  3.1460e-02, -1.0853e-01, -1.3663e-01,\n",
      "         -4.8133e-02, -9.6788e-03, -2.9099e-02, -1.4037e-01, -5.9680e-02,\n",
      "         -1.1300e-01, -1.5524e-01,  1.1915e-01, -1.7076e-01,  6.2006e-02,\n",
      "         -9.4601e-02,  9.2375e-02,  6.5740e-02,  3.9878e-02,  1.4712e-01,\n",
      "          1.7414e-01, -1.4843e-01, -6.4360e-02,  9.0345e-02, -1.0274e-01,\n",
      "          7.0219e-02,  1.2427e-01,  3.7734e-02, -1.0346e-01,  6.2694e-02,\n",
      "         -1.4217e-01,  4.0634e-03,  8.5021e-03,  4.1457e-02,  1.3479e-01,\n",
      "         -1.3365e-01,  1.1143e-01, -1.0527e-02, -1.2008e-01,  3.3626e-02,\n",
      "         -6.4023e-04, -1.7879e-02,  1.3914e-01, -5.4639e-02, -2.9222e-01,\n",
      "         -2.8175e-01, -1.5944e-01, -2.1655e-01,  1.0331e-01,  3.2667e-03,\n",
      "         -7.5504e-02, -1.1773e-01,  2.1844e-02, -1.3085e-01, -1.1704e-02,\n",
      "         -1.8968e-01, -8.0149e-02,  1.0948e-01,  9.0713e-02,  8.6212e-02,\n",
      "          1.5063e-01, -1.2421e-01, -8.3168e-02,  1.0902e-01,  4.8468e-02,\n",
      "          2.2996e-01,  6.8842e-02,  3.9812e-02,  8.1234e-02, -2.1330e-01,\n",
      "          2.0108e-02, -2.0349e-01, -2.0942e-01,  2.2453e-02, -4.5279e-02,\n",
      "         -6.4901e-02, -1.2510e-01,  1.0341e-01, -1.2484e-01, -3.0469e-02,\n",
      "         -1.7358e-01, -1.2043e-01, -2.7654e-02, -2.2308e-01],\n",
      "        [ 2.9792e-02, -6.3624e-02, -1.2453e-01, -3.9854e-02,  4.0713e-02,\n",
      "         -1.6080e-01, -2.1130e-02,  1.4970e-01, -8.2758e-02,  3.6326e-02,\n",
      "         -8.2986e-04,  1.3939e-01, -3.8889e-02,  1.1383e-02,  8.4703e-02,\n",
      "         -2.9431e-02, -2.3087e-01,  1.0794e-01,  5.5276e-02, -3.1940e-02,\n",
      "          5.6189e-02,  9.4105e-02, -1.3250e-01,  8.2542e-02, -2.9436e-01,\n",
      "         -1.0078e-01, -8.5458e-02,  1.1602e-01, -8.9994e-02,  5.6001e-02,\n",
      "          2.3153e-02, -1.7499e-02, -8.7210e-02, -9.3564e-02,  3.1125e-02,\n",
      "         -7.7762e-03, -2.2916e-01, -1.0720e-01,  9.7539e-02, -1.0289e-01,\n",
      "         -1.1048e-03,  4.6022e-02, -3.2585e-02,  1.1973e-01, -2.3688e-01,\n",
      "          1.2350e-01, -3.8888e-02, -4.1825e-02,  5.4122e-02, -3.8897e-02,\n",
      "         -1.5390e-01,  1.0113e-01,  8.3897e-03, -1.2862e-01, -8.3257e-02,\n",
      "          8.4748e-02,  1.9045e-01, -9.4366e-03,  1.0105e-01, -2.0302e-02,\n",
      "          1.3579e-01,  1.0001e-01, -4.5449e-02,  1.3206e-01, -8.3421e-03,\n",
      "         -1.6680e-01,  8.9365e-02, -2.8733e-01,  3.3064e-02,  1.6784e-02,\n",
      "          1.3914e-01,  1.5816e-01, -7.3991e-02, -1.2703e-01, -1.0837e-02,\n",
      "          2.7092e-02, -4.1893e-02,  7.6140e-02, -2.1020e-02, -6.4644e-02,\n",
      "         -1.6600e-01,  1.2811e-01, -8.4966e-02,  1.1306e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for j, i in model.named_parameters():\n",
    "    print(j, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_d1 = torchvision.datasets.MNIST(root = './data',\n",
    "#                                             train = True,\n",
    "#                                             transform = transforms.Compose([\n",
    "#                                                     transforms.Resize((32,32))]),\n",
    "#                                             download = False).data\n",
    "test_d1 = torchvision.datasets.MNIST(root = './data',\n",
    "                                            train = False,\n",
    "                                            transform = transforms.Compose([\n",
    "                                                    transforms.Resize((32,32)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                            download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4267, -0.4267, -0.4267,  ..., -0.4267, -0.4267, -0.4267],\n",
       "          [-0.4267, -0.4267, -0.4267,  ..., -0.4267, -0.4267, -0.4267],\n",
       "          [-0.4267, -0.4267, -0.4267,  ..., -0.4267, -0.4267, -0.4267],\n",
       "          ...,\n",
       "          [-0.4267, -0.4267, -0.4267,  ..., -0.4267, -0.4267, -0.4267],\n",
       "          [-0.4267, -0.4267, -0.4267,  ..., -0.4267, -0.4267, -0.4267],\n",
       "          [-0.4267, -0.4267, -0.4267,  ..., -0.4267, -0.4267, -0.4267]]]),\n",
       " 7)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_d1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.Tensor.numpy(test_d1[0][0]) \n",
    "samptrch = np.zeros((1,1,32,32))\n",
    "samptrch[0] = sample1\n",
    "samptrch = torch.from_numpy(samptrch)\n",
    "# samptrch = torch.from_numpy(samptrch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample1 = np.zeros((1,32,32))\n",
    "# sample1[0][2:30, 2:30] = torch.Tensor.numpy(test_d1[0]) \n",
    "# samptrch = np.zeros((1,1,32,32))\n",
    "# samptrch[0][0][2:30, 2:30] = torch.Tensor.numpy(test_d1[0])\n",
    "# samptrch = torch.from_numpy(samptrch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]]),\n",
       " array([0., 1., 2.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "x2 = np.arange(3.0)\n",
    "x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1 * x2).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv1w = torch.detach(model.conv1.weight_orig).numpy()\n",
    "cnv1b = torch.detach(model.conv1.bias_orig).numpy()\n",
    "cnv2w = torch.detach(model.conv2.weight).numpy()\n",
    "cnv2b = torch.detach(model.conv2.bias).numpy()\n",
    "\n",
    "den1w = torch.detach(model.fc1.weight).numpy()\n",
    "den1b = np.reshape(torch.detach(model.fc1.bias).numpy(), (model.fc1.bias.size()[0], 1)) \n",
    "den2w = torch.detach(model.fc2.weight).numpy()\n",
    "den2b = np.reshape(torch.detach(model.fc2.bias).numpy(), (model.fc2.bias.size()[0], 1)) \n",
    "den3w = torch.detach(model.fc3.weight).numpy()\n",
    "den3b = np.reshape(torch.detach(model.fc3.bias).numpy(), (model.fc3.bias.size()[0], 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den2b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove biases before cross check with conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet_manual():\n",
    "    def __init__(self, conv_layer_1w:np.ndarray, conv_layer_1b:np.ndarray, \n",
    "                 conv_layer_2w:np.ndarray, conv_layer_2b:np.ndarray,\n",
    "                 dense_1_w:np.ndarray, dense_1_b:np.ndarray, \n",
    "                 dense_2_w:np.ndarray, dense_2_b:np.ndarray,\n",
    "                 dense_3_w:np.ndarray, dense_3_b:np.ndarray):\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = self.Conv2d(1, 6, 5, conv_layer_1w, conv_layer_1b)\n",
    "        self.conv2 = self.Conv2d(6, 16, 5, conv_layer_2w, conv_layer_2b)\n",
    "        self.fc1 = self.myDense2d(dense_1_w, dense_1_b)\n",
    "        self.fc2 = self.myDense2d(dense_2_w, dense_2_b)\n",
    "        self.fc3 = self.myDense2d(dense_3_w, dense_3_b)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "    #     x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "    #     x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     x = F.relu(self.fc2(x))\n",
    "    #     x = self.fc3(x)\n",
    "    #     return x\n",
    "    \n",
    "    def Conv2d(self, inChan:int, outChan:int, kernalDim:int, weight: np.ndarray, bias: np.ndarray):\n",
    "        myIn = inChan\n",
    "        myOt = outChan\n",
    "        myKr = kernalDim\n",
    "        myWeight = weight\n",
    "        myBias = bias\n",
    "\n",
    "        def CalConv2d(img:np.ndarray) -> np.ndarray:\n",
    "            inDim, imdim_r, imdim_c = img.shape\n",
    "            outImg = np.zeros((myOt, imdim_r-myKr, imdim_c-myKr))\n",
    "            for oc in range(myOt):\n",
    "                bias_s = myBias[oc]\n",
    "                for ic in range(myIn):\n",
    "                    kernal = myWeight[oc][ic]\n",
    "                    for kr_i in range(int(imdim_r-myKr)):\n",
    "                        for kr_j in range(int(imdim_c-myKr)):\n",
    "                            outImg[oc][kr_i][kr_j] += (kernal * img[ic][kr_i:kr_i+myKr,kr_j:kr_j+myKr]).sum() + bias_s\n",
    "            return outImg\n",
    "        return CalConv2d\n",
    "    \n",
    "    def myRelu(self, img:np.ndarray) -> np.ndarray:\n",
    "        return img.clip(0)\n",
    "    \n",
    "    def maxPool2d(self, img:np.ndarray) -> np.ndarray:\n",
    "        inDim, imdim_r, imdim_c = img.shape\n",
    "        outImg = np.zeros_like(img, shape=(inDim, int((imdim_r+1)/2), int((imdim_c+1)/2)))\n",
    "        for chn in range(inDim):\n",
    "            for i in range(int((imdim_r+1)/2)):\n",
    "                for j in range(int((imdim_c+1)/2)):\n",
    "                    if(i*2 > imdim_r):\n",
    "                        if(j*2 > imdim_c):\n",
    "                            outImg[chn][i,j] = img[chn][i*2,j*2]\n",
    "                        else:\n",
    "                            outImg[chn][i,j] = (img[chn][i*2,j*2:j*2+2]).max()\n",
    "                    else:\n",
    "                        if(j*2 > imdim_c):\n",
    "                            outImg[chn][i,j] = (img[chn][i*2:i*2+2,j*2]).max()\n",
    "                        else:\n",
    "                            outImg[chn][i,j] = (img[chn][i*2:i*2+2,j*2:j*2+2]).max()\n",
    "        return outImg\n",
    "\n",
    "    def myReshape(self, img:np.ndarray) -> np.ndarray:\n",
    "        return np.reshape(img, (img.size, -1))\n",
    "    \n",
    "    def myDense2d(self, weight:np.ndarray, bias:np.ndarray) -> np.ndarray:\n",
    "        myweight = weight\n",
    "        mybias  = bias\n",
    "\n",
    "        def dense(img:np.ndarray):\n",
    "            return np.dot(myweight, img) +  mybias\n",
    "\n",
    "        return dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = LeNet_manual(cnv1w, cnv1b, cnv2w, cnv2b, den1w, den1b, den2w, den2b, den3w, den3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trch_forward(x):\n",
    "    x = F.max_pool2d(F.relu(F.conv2d(x, weight=model.conv1.weight_orig, bias=model.conv1.bias_orig)), (2, 2))\n",
    "    x = F.max_pool2d(F.relu(F.conv2d(x, weight=model.conv2.weight, bias=model.conv2.bias)), 2)\n",
    "    x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "    x = F.relu(F.linear(x, weight=model.fc1.weight, bias=model.fc1.bias))\n",
    "    x = F.relu(F.linear(x, weight=model.fc2.weight, bias=model.fc2.bias))\n",
    "    x = F.linear(x, weight=model.fc3.weight, bias=model.fc3.bias)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_forward(x):\n",
    "    x = mm.maxPool2d(mm.myRelu(mm.conv1(x)))\n",
    "    x = mm.maxPool2d(mm.myRelu(mm.conv2(x)))\n",
    "    x = mm.myReshape(x)\n",
    "    x = mm.myRelu(mm.fc1(x))\n",
    "    x = mm.myRelu(mm.fc2(x))\n",
    "    x = mm.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 86.44 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = trch_forward(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 84.75 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    \n",
    "with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = torch.zeros(images.shape[0])\n",
    "            for i in range(images.shape[0]):\n",
    "                  outputs[i] = np.argmax(my_forward(torch.Tensor.numpy(images[i])))\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "      print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
