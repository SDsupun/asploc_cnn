{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project ASPLOC - 1DT109 11201 (2024HT)\n",
    "#### By Supun Madusanka\n",
    "\n",
    "This script will use the LeNet NN architecture and quantize it to be used on hardware accelerator\n",
    "\n",
    "Ref:\n",
    "- [PyTorch tutorial on NN](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "- [PyTorch tutorial on NN pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n",
    "- [CNN](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "- [NN quantization](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requried libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import serial\n",
    "\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "DOWNLOAD_DATA = False\n",
    "DATA_PATH = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = DATA_PATH,\n",
    "                                            train = True,\n",
    "                                            transform = transforms.Compose([\n",
    "                                                    transforms.Resize((32,32)),\n",
    "                                                    transforms.ToTensor()\n",
    "                                                #     ,\n",
    "                                                #     transforms.Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "                                                    ]),\n",
    "                                            download = DOWNLOAD_DATA)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = DATA_PATH,\n",
    "                                            train = False,\n",
    "                                            transform = transforms.Compose([\n",
    "                                                    transforms.Resize((32,32)),\n",
    "                                                    transforms.ToTensor()\n",
    "                                                #     ,\n",
    "                                                #     transforms.Normalize(mean = (0.1325,), std = (0.3105,))\n",
    "                                                    ]),\n",
    "                                            download=DOWNLOAD_DATA)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model with PyTorch\n",
    "==============\n",
    "\n",
    "In this tutorial, we use the\n",
    "[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture\n",
    "from LeCun et al., 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    ".format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'lenet_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the training if already done and restart from here by loading existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('lenet_model.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unwrap model functions for quantization\n",
    "=======================================\n",
    "\n",
    "Here we will unwrap most of the functions for quantization and make it hardware friendly. The trained weights and biases from previous model is used inside the functions and the training phase is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize(X:np.matrix) -> np.matrix:\n",
    "    # Calculate scale\n",
    "    scale = 127 / np.max(np.absolute(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = np.round(scale * X)\n",
    "\n",
    "    return X_quant.astype(np.int16), scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs256_quantize(X:np.matrix) -> np.matrix:\n",
    "    # Fix scale\n",
    "    scale = 256\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = np.round(scale * X)\n",
    "\n",
    "    return X_quant.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs256_dequantize(X:np.matrix) -> np.matrix:\n",
    "    # Fix scale\n",
    "    scale = 256\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = np.round(X/scale)\n",
    "\n",
    "    return X_quant.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs4_quantize(X:np.matrix) -> np.matrix:\n",
    "    # Fix scale\n",
    "    scale = 4\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = np.round(scale * X)\n",
    "\n",
    "    return X_quant.astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the scaling factors returned by absmax_quantize is close to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_quantize(torch.detach(model.conv1.weight).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is the breakdown of the functionalities used above with the Torch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet_quantize():\n",
    "    def __init__(self, conv_layer_1w:np.ndarray, conv_layer_1b:np.ndarray, \n",
    "                 conv_layer_2w:np.ndarray, conv_layer_2b:np.ndarray,\n",
    "                 dense_1_w:np.ndarray, dense_1_b:np.ndarray, \n",
    "                 dense_2_w:np.ndarray, dense_2_b:np.ndarray,\n",
    "                 dense_3_w:np.ndarray, dense_3_b:np.ndarray):\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = self.Conv2d(1, 6, 5, conv_layer_1w, conv_layer_1b)\n",
    "        self.conv2 = self.Conv2d(6, 16, 5, conv_layer_2w, conv_layer_2b)\n",
    "        self.fc1 = self.myDense2d(dense_1_w, dense_1_b)\n",
    "        self.fc2 = self.myDense2d(dense_2_w, dense_2_b)\n",
    "        self.fc3 = self.myDense2d(dense_3_w, dense_3_b)\n",
    "    \n",
    "    def Conv2d(self, inChan:int, outChan:int, kernalDim:int, weight: np.ndarray, bias: np.ndarray):\n",
    "        myIn = inChan\n",
    "        myOt = outChan\n",
    "        myKr = kernalDim\n",
    "        myWeight = weight\n",
    "        myBias = bias\n",
    "\n",
    "        def CalConv2d(img:np.ndarray) -> np.ndarray:\n",
    "            inDim, imdim_r, imdim_c = img.shape\n",
    "            outImg = np.zeros((myOt, imdim_r-myKr, imdim_c-myKr))\n",
    "            for oc in range(myOt):\n",
    "                bias_s = myBias[oc]\n",
    "                for ic in range(myIn):\n",
    "                    kernal = myWeight[oc][ic]\n",
    "                    for kr_i in range(int(imdim_r-myKr)):\n",
    "                        for kr_j in range(int(imdim_c-myKr)):\n",
    "                            outImg[oc][kr_i][kr_j] += (kernal * img[ic][kr_i:kr_i+myKr,kr_j:kr_j+myKr]).sum() + bias_s\n",
    "            return outImg\n",
    "        return CalConv2d\n",
    "    \n",
    "    def myRelu(self, img:np.ndarray) -> np.ndarray:\n",
    "        return img.clip(0)\n",
    "    \n",
    "    def maxPool2d(self, img:np.ndarray) -> np.ndarray:\n",
    "        inDim, imdim_r, imdim_c = img.shape\n",
    "        outImg = np.zeros_like(img, shape=(inDim, int((imdim_r+1)/2), int((imdim_c+1)/2)))\n",
    "        for chn in range(inDim):\n",
    "            for i in range(int((imdim_r+1)/2)):\n",
    "                for j in range(int((imdim_c+1)/2)):\n",
    "                    if(i*2 > imdim_r):\n",
    "                        if(j*2 > imdim_c):\n",
    "                            outImg[chn][i,j] = img[chn][i*2,j*2]\n",
    "                        else:\n",
    "                            outImg[chn][i,j] = (img[chn][i*2,j*2:j*2+2]).max()\n",
    "                    else:\n",
    "                        if(j*2 > imdim_c):\n",
    "                            outImg[chn][i,j] = (img[chn][i*2:i*2+2,j*2]).max()\n",
    "                        else:\n",
    "                            outImg[chn][i,j] = (img[chn][i*2:i*2+2,j*2:j*2+2]).max()\n",
    "        return outImg\n",
    "\n",
    "    def myReshape(self, img:np.ndarray) -> np.ndarray:\n",
    "        return np.reshape(img, (img.size, -1))\n",
    "    \n",
    "    def myDense2d(self, weight:np.ndarray, bias:np.ndarray) -> np.ndarray:\n",
    "        myweight = weight\n",
    "        mybias  = bias\n",
    "\n",
    "        def dense(img:np.ndarray):\n",
    "            return np.dot(myweight, img) +  mybias\n",
    "\n",
    "        return dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights and biases from the model and inspect if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv1w = torch.detach(model.conv1.weight).numpy()\n",
    "cnv1b = torch.detach(model.conv1.bias).numpy()\n",
    "cnv2w = torch.detach(model.conv2.weight).numpy()\n",
    "cnv2b = torch.detach(model.conv2.bias).numpy()\n",
    "\n",
    "den1w = torch.detach(model.fc1.weight).numpy()\n",
    "den1b = np.reshape(torch.detach(model.fc1.bias).numpy(), (model.fc1.bias.size()[0], 1)) \n",
    "den2w = torch.detach(model.fc2.weight).numpy()\n",
    "den2b = np.reshape(torch.detach(model.fc2.bias).numpy(), (model.fc2.bias.size()[0], 1)) \n",
    "den3w = torch.detach(model.fc3.weight).numpy()\n",
    "den3b = np.reshape(torch.detach(model.fc3.bias).numpy(), (model.fc3.bias.size()[0], 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed scale (256) quantized weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv1wq = fs256_quantize(torch.detach(model.conv1.weight).numpy())\n",
    "cnv1bq = fs256_quantize(torch.detach(model.conv1.bias).numpy())\n",
    "cnv2wq = fs256_quantize(torch.detach(model.conv2.weight).numpy())\n",
    "cnv2bq = fs256_quantize(torch.detach(model.conv2.bias).numpy())\n",
    "\n",
    "den1wq = fs256_quantize(torch.detach(model.fc1.weight).numpy())\n",
    "den1bq = fs256_quantize(np.reshape(torch.detach(model.fc1.bias).numpy(), (model.fc1.bias.size()[0], 1)) )\n",
    "den2wq = fs256_quantize(torch.detach(model.fc2.weight).numpy())\n",
    "den2bq = fs256_quantize(np.reshape(torch.detach(model.fc2.bias).numpy(), (model.fc2.bias.size()[0], 1)) )\n",
    "den3wq = fs256_quantize(torch.detach(model.fc3.weight).numpy())\n",
    "den3bq = fs256_quantize(np.reshape(torch.detach(model.fc3.bias).numpy(), (model.fc3.bias.size()[0], 1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH+'/np_data/cnv1wq.npy', 'wb') as f:\n",
    "    np.save(f, cnv1wq)\n",
    "with open(DATA_PATH+'/np_data/cnv1bq.npy', 'wb') as f:\n",
    "    np.save(f, cnv1bq)\n",
    "with open(DATA_PATH+'/np_data/cnv2wq.npy', 'wb') as f:\n",
    "    np.save(f, cnv2wq)\n",
    "with open(DATA_PATH+'/np_data/cnv2bq.npy', 'wb') as f:\n",
    "    np.save(f, cnv2bq)\n",
    "with open(DATA_PATH+'/np_data/den1wq.npy', 'wb') as f:\n",
    "    np.save(f, den1wq)\n",
    "with open(DATA_PATH+'/np_data/den1bq.npy', 'wb') as f:\n",
    "    np.save(f, den1bq)\n",
    "with open(DATA_PATH+'/np_data/den2wq.npy', 'wb') as f:\n",
    "    np.save(f, den2wq)\n",
    "with open(DATA_PATH+'/np_data/den2bq.npy', 'wb') as f:\n",
    "    np.save(f, den2bq)\n",
    "with open(DATA_PATH+'/np_data/den3wq.npy', 'wb') as f:\n",
    "    np.save(f, den3wq)\n",
    "with open(DATA_PATH+'/np_data/den3bq.npy', 'wb') as f:\n",
    "    np.save(f, den3bq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unquantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = LeNet_quantize(cnv1w, cnv1b, cnv2w, cnv2b, den1w, den1b, den2w, den2b, den3w, den3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmq = LeNet_quantize(cnv1wq, cnv1bq, cnv2wq, cnv2bq, den1wq, den1bq, den2wq, den2bq, den3wq, den3bq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward function LeNet class is copied here. \n",
    "- This is used to cross-validate the individual layer outputs with the unwraped LeNet_quantize class. Keep the required layer to cross-check and comment the other layers and check the output as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trch_forward(x):\n",
    "    x = F.max_pool2d(F.relu(F.conv2d(x, weight=model.conv1.weight, bias=model.conv1.bias)), (2, 2))\n",
    "    x = F.max_pool2d(F.relu(F.conv2d(x, weight=model.conv2.weight, bias=model.conv2.bias)), 2)\n",
    "    x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "    x = F.relu(F.linear(x, weight=model.fc1.weight, bias=model.fc1.bias))\n",
    "    x = F.relu(F.linear(x, weight=model.fc2.weight, bias=model.fc2.bias))\n",
    "    x = F.linear(x, weight=model.fc3.weight, bias=model.fc3.bias)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correspondencing unwraped function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_forward(x):\n",
    "    x = mm.maxPool2d(mm.myRelu(mm.conv1(x)))\n",
    "    x = mm.maxPool2d(mm.myRelu(mm.conv2(x)))\n",
    "    x = mm.myReshape(x)\n",
    "    x = mm.myRelu(mm.fc1(x))\n",
    "    x = mm.myRelu(mm.fc2(x))\n",
    "    x = mm.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correspondencing unwraped quantized function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_q_forward(x):\n",
    "    x = mmq.maxPool2d(mmq.myRelu(mmq.conv1(x)))\n",
    "    x = fs256_dequantize(x)\n",
    "    x = mmq.maxPool2d(mmq.myRelu(mmq.conv2(x)))\n",
    "    x = fs256_dequantize(x)\n",
    "    x = mmq.myReshape(x)\n",
    "    x = mmq.myRelu(mmq.fc1(x))\n",
    "    x = fs256_dequantize(x)\n",
    "    x = mmq.myRelu(mmq.fc2(x))\n",
    "    x = fs256_dequantize(x)\n",
    "    x = mmq.fc3(x)\n",
    "    x = fs256_dequantize(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = trch_forward(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate unwraped model\n",
    "\n",
    "> [Note] my_forward() function is incredibly slow compared to trch_forward()\n",
    "\n",
    "- Obs. my_forward does not support batch input hence the for loop.\n",
    "- Obs. a bit of accuracy drop due to no padding and other missing small fine tunes that present in Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    \n",
    "with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = torch.zeros(images.shape[0])\n",
    "            for i in range(images.shape[0]):\n",
    "                  outputs[i] = np.argmax(my_forward(torch.Tensor.numpy(images[i])))\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "      print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate unwraped (FS) quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    \n",
    "with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = torch.zeros(images.shape[0])\n",
    "            for i in range(images.shape[0]):\n",
    "                  outputs[i] = np.argmax(my_q_forward(fs4_quantize(torch.Tensor.numpy(images[i]))))\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "      print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use QLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qLenet import QLeNet\n",
    "mmn = QLeNet()\n",
    "mmn.qLenet_forward(fs4_quantize(torch.Tensor.numpy(images[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tohex(val, nbits):\n",
    "  hex_s = hex((val + (1 << nbits)) % (1 << nbits))\n",
    "  return hex_s.split(\"x\")[1].rjust(2, '0')\n",
    "\n",
    "def save_q_val(fname, data):\n",
    "    with open(fname, 'w') as fout:\n",
    "        for chni in data:\n",
    "            if(type(chni) == np.int16):\n",
    "                val = tohex(chni, 8)\n",
    "                fout.write(f'{val}\\n')\n",
    "            else:\n",
    "                for ki in chni:\n",
    "                    if(type(ki) == np.int16):\n",
    "                        val = tohex(ki, 8)\n",
    "                        fout.write(f'{val}\\n')\n",
    "                    else:\n",
    "                        for row in ki:\n",
    "                            if(type(row) == np.int16):\n",
    "                                val = tohex(row, 8)\n",
    "                                fout.write(f'{val}\\n')\n",
    "                            else:\n",
    "                                for col in row:\n",
    "                                    val = tohex(col, 8)\n",
    "                                    fout.write(f'{val}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_q_val(\"../wnb/conv1_weight.mem\", cnv1wq)\n",
    "save_q_val(\"../wnb/conv1_bias.mem\", cnv1bq)\n",
    "save_q_val(\"../wnb/conv2_weight.mem\", cnv2wq)\n",
    "save_q_val(\"../wnb/conv2_bias.mem\", cnv2bq)\n",
    "save_q_val(\"../wnb/fs1_weight.mem\", den1wq)\n",
    "save_q_val(\"../wnb/fs1_bias.mem\", den1bq)\n",
    "save_q_val(\"../wnb/fs2_weight.mem\", den2wq)\n",
    "save_q_val(\"../wnb/fs2_bias.mem\", den2bq)\n",
    "save_q_val(\"../wnb/fs3_weight.mem\", den3wq)\n",
    "save_q_val(\"../wnb/fs3_bias.mem\", den3bq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_q_uart_str(data) -> bytearray:\n",
    "    image_str:str = ''\n",
    "    for chni in data:\n",
    "        if(type(chni) == np.int16):\n",
    "            val = tohex(chni, 8)\n",
    "            image_str += f'{val}'\n",
    "        else:\n",
    "            for ki in chni:\n",
    "                if(type(ki) == np.int16):\n",
    "                    val = tohex(ki, 8)\n",
    "                    image_str += f'{val}'\n",
    "                else:\n",
    "                    for row in ki:\n",
    "                        if(type(row) == np.int16):\n",
    "                            val = tohex(row, 8)\n",
    "                            image_str += f'{val}'\n",
    "                        else:\n",
    "                            for col in row:\n",
    "                                val = tohex(col, 8)\n",
    "                                image_str += f'{val}'\n",
    "    return bytearray.fromhex(image_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the MINIZED and run this. change the COM port to the connected port\n",
    "uartConnection = serial.Serial(\"COM8\", 115200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hw(img_test: bytearray) -> int:\n",
    "    # Test the loopback\n",
    "    uartConnection.write(img_test)\n",
    "    hw_pred:bytearray\n",
    "    # Read line   \n",
    "    while True:\n",
    "        hw_pred = uartConnection.readline()\n",
    "        try:\n",
    "            hw_say = hw_pred.decode(\"utf-8\")\n",
    "        except: UnicodeDecodeError\n",
    "        \n",
    "        if('Predict Result' in hw_say):\n",
    "            return int(hw_say.split()[-1], 16)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = torch.zeros(images.shape[0])\n",
    "            for i in range(images.shape[0]):\n",
    "                  img_q4 = fs4_quantize(torch.Tensor.numpy(images[i]))\n",
    "                  pred_sw_o = np.argmax(my_q_forward(img_q4))\n",
    "                  pred_hw_o = predict_hw(image_q_uart_str(img_q4))\n",
    "                  print(f'Label: {labels[i]}, SW Pred: {pred_sw_o}, HW Pred: {pred_hw_o}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction time is largely limited by the UART connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images, labels in test_loader:\n",
    "            outputs = torch.zeros(images.shape[0])\n",
    "            for i in range(images.shape[0]):\n",
    "                  img_q4 = fs4_quantize(torch.Tensor.numpy(images[i]))\n",
    "                  outputs[i] = predict_hw(image_q_uart_str(img_q4))\n",
    "                  print(f'{labels[i]}, {outputs[i]}')\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "      print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
